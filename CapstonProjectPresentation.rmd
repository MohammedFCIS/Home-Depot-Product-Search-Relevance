---
title: "Home Depot Product Search Relevance"
author: "Mohammed Ali"
date: "March 13, 2016"
output: ioslides_presentation
---

## Agenda

- Overview
- Data Story
- Modeling
- Output Analysis


## Overview
- Home Depot Product Search Relevance is Kaggle competition targets to improve Home Depot customers' shopping experience.
  - Target: developing a model that can accurately predict the relevance of search results.
![home_depot_tools.jpg](home_depot_tools.jpg)

##Data Story

- This data set contains a number of products and real customer search terms from Home Depot's website.
- The challenge: to predict a relevance score for the provided combinations of search terms and products.
- The relevance is a number between **1** (not relevant) to **3** (highly relevant). 

For example, a search for "AA battery" would be considered highly relevant to a pack of size AA batteries (relevance = 3), mildly relevant to a cordless drill battery (relevance = 2), and not relevant to a snow shovel (relevance = 1).

##Data Story Cont..
```{r, echo=FALSE}
suppressMessages(library(reader))
suppressMessages(library(dplyr))
suppressMessages(library(tidyr))
suppressMessages(library(ggplot2)) 
suppressMessages(library(gridExtra))
```
Data includes the following files:

- ***train.csv*** --> the training set, contains products, searches, and relevance scores.
```{r, echo=FALSE}
# Read training data first
products_training <- tbl_df(read.csv("data/train.csv", stringsAsFactors = FALSE))
# Have a quick glimpese on it
glimpse(products_training)
```

##Data Story Cont..
-  Relevance Distribution
```{r, echo=FALSE}
# Read relevance data
relevance_dist <- products_training %>% count(as.factor(relevance))  %>% arrange(desc(n))
#Let us explore the distribution
suppressMessages(ggplot(products_training, aes(x = relevance)) +
   geom_histogram(color = "black", fill = "DarkOrange") +
  scale_x_continuous(breaks = seq(0, 3, 0.2)))
```

##Data Story Cont..
- Relevance Desnisty Function
```{r, echo=FALSE}
# Desnisty function
ggplot(products_training, aes(x = relevance)) +
   geom_density(color = "black") +
  scale_x_continuous(breaks = seq(0, 3, 0.2))
```

##Data Story Cont..
- ***test.csv*** --> similar to train.csv except the absence for relevance scores.
```{r, echo=FALSE}
# Desnisty function
products_test <- tbl_df(read.csv("data/test.csv",stringsAsFactors = FALSE))
#combine training and test data, Joining by: c("id", "product_uid", "product_title", "search_term")
suppressMessages(product_training_test<- full_join(products_training,products_test))
```
- ***product_descriptions.csv*** contains a text description of each product.
```{r, echo=FALSE}
product_description <- tbl_df(read.csv("data/product_descriptions.csv",stringsAsFactors = FALSE))
glimpse(product_description)
```
##Data Story Cont..
- Combined train, test and product desription data
```{r, echo=FALSE}
#combine test data and data combined at the previous steps,Joining by: c("product_uid")
suppressMessages(product_all <- right_join(product_training_test, product_description))
#data after merging training, test and description datasets
glimpse(product_all)
```
##Data Story Cont..
-  The most search terms used
```{r, echo=FALSE}
#Now let us find out the most searched terms
product_all %>% count(search_term)  %>% arrange(desc(n))

```
##Data Story Cont..
-  The most searched products
```{r, echo=FALSE}
product_all %>% count(product_uid)  %>% arrange(desc(n))

```

##Data Story Cont..
- ***attributes.csv***  provides extended information about a subset of the products (typically representing detailed technical specifications). Not every product is having attributes

```{r, echo=FALSE}
#Read the attributes of products
products_attributes <- tbl_df(read.csv("data/attributes.csv",stringsAsFactors = FALSE, na.strings = 'N/A'))
glimpse(products_attributes)
```

##Modeling
- *Step 1*: combine attributes keys and values
```{r, echo=FALSE}
products_attributes <- products_attributes  %>%
                       filter(product_uid != 'NA') %>% #revmove null rows
                       unite(property, c(name,value), sep = ';;')   #combine name and values columns
glimpse(products_attributes)
```

##Modeling Cont...
- *Step 2*: group rows with the same id
```{r, echo=FALSE}
# group rows with the same id
products_attributes <- aggregate(products_attributes$property ~ products_attributes$property, by=list(products_attributes$product_uid), FUN=paste, collapse="@@@@")
glimpse(products_attributes)
```

##Modeling Cont...
- *Step 3*: restore original names
```{r, echo=FALSE}
#restore original names
products_attributes <- products_attributes %>% rename(product_uid = Group.1, property = `products_attributes$property`)
glimpse(products_attributes)
```

##Modeling Cont...
- *Step 4*: Generate new attribute fields and combine with the main product set 
```{r, echo=FALSE}
source('attributesParser.R')
products_attributes <- mutate(products_attributes, bullets = sapply(property, FUN = bulletsParser),
                                                   yeses = sapply(property, FUN = yesesParser),
                                                   nos = sapply(property, FUN = nosParser),
                                                   keys = sapply(property, FUN = keysParser),
                                                   values = sapply(property, FUN = valuesParser))
```

```{r, echo=FALSE}
#merge attributes with the main dataset
product_all <- full_join(product_all, products_attributes)
glimpse(product_all)
```
##Modeling Cont...
- *Step 5*: Generate the features that is used in linear regression
```{r, echo=FALSE}
source('matchScorer.R')
product_all <- mutate(product_all, bulletsScore = mapply(phrasesMatchScore, search_term, bullets),
                                   yesesScore   = mapply(phrasesMatchScore, search_term, yeses),
                                   nosScore     = mapply(phrasesMatchScore, search_term, nos),
                                   keysScore    = mapply(phrasesMatchScore, search_term, keys),
                                   valuesScore  = mapply(phrasesMatchScore, search_term, values))
glimpse(product_all)
```
##Modeling Cont...
- *Step 6*:  Divide the data into training and test sets so can perform prediction and test our model
```{r, echo=FALSE}
product_all_train <- subset(product_all, !is.na(relevance))
product_all_test  <- subset(product_all, is.na(relevance))
```
- *Step 7*: Performing linear regression
```{r, echo=FALSE}
RegModel = lm(relevance ~ bulletsScore + yesesScore + nosScore + keysScore + valuesScore, data = product_all_train)
TestPredictions = predict(RegModel, newdata = product_all_test)
```
- *Step 8*: Test model investigation
```{r, echo=FALSE}
summary(RegModel)
```

## Output Analysis
- Relevance Distribution
```{r, echo=FALSE}
# Read relevance data
product_all_test$relevance <- as.numeric(TestPredictions)
relevance_dist_final <- product_all_test %>% count(as.factor(relevance))  %>% arrange(desc(n))
#Let us explore the distribution
ggplot(product_all_test, aes(x = relevance)) +
   geom_histogram(color = "black", fill = "DarkOrange", bins = 12) +
  scale_x_continuous(breaks = seq(2, 3, 0.1))
```

## Output Analysis Cont ...
- We could not help but notice that data is spread between **2.2** and **2.6** with max value is between **2.3** and **2.4**.
- The data is, nearly, normally distributed with an outlier in **2.5**.
- Let us see another view
```{r, echo=FALSE}
# Desnisty function
relecance_graph <- ggplot(product_all_test, aes(x = relevance)) +
   geom_density(color = "black") +
  scale_x_continuous(breaks = seq(2, 3, 0.2))
```
It confirms our previous induction.

## Output Analysis Cont ...
- Let us now investigate the features used in the model to see their affect on the result
```{r, echo=FALSE}

bullets_graph <- ggplot(product_all_test, aes(x = bulletsScore))+
   geom_density(color = "black") +
   scale_x_continuous()
yeses_graph <- ggplot(product_all_test, aes(x = yesesScore))+
   geom_density(color = "black") +
   scale_x_continuous()
nos_graph <- ggplot(product_all_test, aes(x = nosScore))+
   geom_density(color = "black") +
   scale_x_continuous()
keys_graph <- ggplot(product_all_test, aes(x = keysScore))+
   geom_density(color = "black") +
   scale_x_continuous()
values_graph <- ggplot(product_all_test, aes(x = valuesScore))+
   geom_density(color = "black") +
   scale_x_continuous()
grid.arrange(bullets_graph, yeses_graph, nos_graph, keys_graph, values_graph, relecance_graph)

```

It seems that the builets score is one affects the outlier.

## Output Analysis Cont ...
- Now, let us investigate more by seeing what each feature behave against relevance
```{r, echo=FALSE}
bullets_rel <- ggplot(product_all_test, aes(x = relevance, y = bulletsScore))+
  geom_point()

yeses_rel <- ggplot(product_all_test, aes(x = relevance, y = yesesScore))+
  geom_point()

nos_rel <- ggplot(product_all_test, aes(x = relevance, y = nosScore))+
  geom_point()

keys_rel <- ggplot(product_all_test, aes(x = relevance, y = keysScore))+
  geom_point()

values_rel <- ggplot(product_all_test, aes(x = relevance, y = valuesScore))+
  geom_point()

grid.arrange(bullets_rel, yeses_rel, nos_rel, keys_rel, values_rel)
```

and it seems the value score the one drive the relevance
